# OpenShift On OpenShift (O^3) Deployment Guide

I recommend reading my blog post on [OpenShift on OpenShift](https://www.openshift.com/blog/nested-openshift-using-openshift-virtualization) to get familiar with the architecture and terminology before reading this guide. Once you read this article and are ready to deploy your own cluster, I recommend trying out the Ansible playbook in this repo

## Pre Requisites

This guide assumes the following.

- You have an existing OpenShift 4.6+ cluster with OpenShift Virtualization installed
- Have cluster admin access to said cluster.
- Have access to the DNS, DHCP and HTTP services used to deploy the infrastructure cluster.

### DHCP

Since we need to have the MAC addresses defined in DHCP before we deploy the OCP Nodes, we can't use the MACs generated by OpenShift Virtualization at deploy time. We need to generated the MAC addresses to be used beforehand. You can use the mac generator script found [here](https://github.com/arvin-a/openshift-upi-automation/tree/master/src/scripts) to generate random MAC addresses for your nodes.

```Shell
$ ./generate-mac.sh
02:00:00:2c:86:e7
02:00:00:41:61:98
02:00:00:fb:84:c8
02:00:00:8d:83:41
...
```

Use the generated MACs to add entries for your cluster nodes to your DHCP server.

### Load Balancer

Create new load balancer entries for the O^3 cluster using the IPs defined in your DHCP service for the O^3 VMs. Ports 22623 and 6443 need to be load balanced against the IPs chosen for the control (master) nodes. Ports 80 and 443 need to be load balanced against the IPs chosen for the worker nodes. Take note of the IP or VIP of the load balancer for the DNS section below.

### DNS

Add the required DNS entries for your O^3 cluster (api, api-int, apps pointing to the IP of the load balancer and node host names pointing to their respective IPs)

So for the api endpoint of your cluster, if your load balancer IP is 10.1.1.10, the DNS entry for api.cluster-name.domain.com would point to that address. You would do the same for api-int and apps.

## Infrastructure Cluster Setup

Create a new namespace for the O^3 cluster. This is where the VMs will be deployed to.

```Shell
oc create ns ocp-tenant1
```

We will be using the OpenStack RHCOS image for our O^3 nodes. Download the the image from https://mirrror.openshift.com and unzip it. Then run the following command to upload the image to a DataVolume. We will be using this DV as a source to clone the volumes of the RHCOS nodes. In the command below I am using OpenShift Container Storage installed on the infrastructure cluster. If you have that installed (good choice) you can use the command as is. If not, substitute the storage class with one available in your infrastructure cluster. Please note, fast storage is required to satisfy the IOPS and latency requirements of etcd.

```Shell
virtctl image-upload dv rhcos-dv --size=120Gi \
--storage-class=ocs-storagecluster-ceph-rbd \
--image-path=./rhcos-openstack.x86_64.qcow2 \
--namespace ocp-tenant1 --insecure \
--block-volume --access-mode ReadWriteMany
```

We set the DV size to 120Gi because that's the desired size of the drives for the VMs we want to create. The drive size for cloned DVs must be the same as the source. Wait until DV upload is complete.

### Create Network Bridge

The O^3 cluster VMs need to use the host network. Hence we have to create a network bridge on the infra cluster nodes. You can do that easily using Nmstate via Node Network Configuration Policy (NNCP) objects.

``` Yaml
apiVersion: nmstate.io/v1alpha1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: br1
spec:
  nodeSelector: 
    node-role.kubernetes.io/worker: ""
  desiredState:
    interfaces:
      - name: br1
        description: Linux bridge using enp3s0 device
        type: linux-bridge
        state: up
        ipv4:
          dhcp: true
          enabled: true
        bridge:
          options:
            stp:
              enabled: false
          port:
            - name: enp3s0 # change to the device name of the nic being used on the infra nodes
```

Find the device name of the NIC you want to bridge on the the infra cluster nodes and updated the name of the device in the YAML. You can use the primary NIC but it is recommended you use a secondary NIC. Apply the NNCP YAML above and make sure the bridge is configured

```Shell
$ oc get nncp
NAME   STATUS
br1    SuccessfullyConfigured
```

To use the bridge defined above, you have to define a Network Attachment Definition in the namespace where you will be deploying the 0^3 cluster VMs. Apply the YAML below to create the NAD for your namespace.

```Yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    k8s.v1.cni.cncf.io/resourceName: bridge.network.kubevirt.io/br1
  name: br1
  namespace: ocp-tenant1
spec:
  config: >-
    {"name":"br1","cniVersion":"0.3.1","plugins":[{"type":"cnv-bridge","bridge":"br1","ipam":{}},{"type":"cnv-tuning"}]}
```

### Generate And Stage Ignition Files

The ignition files will be fed into the VMs via a Cloud Init which will take the data from a secret. Sometimes the bootstrap ignition content size can exceed the the limit of secrets. Due to this, we will be using a pointer ignition that will point to the actual ignition on a web server.

1. Run the OpenShift installer to generate ignition files
2. Copy the bootstrap ignition to your web server
3. Create a new bootstrap ignition that will point to the one hosted on your web server

```Json
{
    "ignition": {
        "config": {
            "merge": [
                {
                    "source": "http://MyHttpServer/PathToFile/bootstrap.ign"
                }
            ]
        },
        "security": {},
        "version": "3.1.0"
    }
}
```

4. Create secrets from the ignition files generated for the master and worker nodes. For bootstrap use the pointer file described above instead.

Example:

```Shell
oc create secret generic worker-ign \
--from-file=userdata=/ocp_install_dir/worker.ign \
-n ocp-tenant1
```

### Define VM spec

Define the VM spec for each O^3 node.

```Yaml
apiVersion: kubevirt.io/v1alpha3
kind: VirtualMachine
metadata:
  annotations:
    kubevirt.io/latest-observed-api-version: v1alpha3
    kubevirt.io/storage-observed-api-version: v1alpha3
    name.os.template.kubevirt.io/rhel8.3: Red Hat Enterprise Linux 8.0 or higher
  name: control1
  namespace: {{ ocp_cluster_name }}
  labels:
    app: control1
    os.template.kubevirt.io/rhel8.3: 'true'
    vm.kubevirt.io/template: rhel8-server-large-v0.11.3
    vm.kubevirt.io/template.namespace: openshift
    vm.kubevirt.io/template.revision: '1'
    vm.kubevirt.io/template.version: v0.12.3
    workload.template.kubevirt.io/server: 'true'
spec:
  dataVolumeTemplates:
    - apiVersion: cdi.kubevirt.io/v1alpha1
      kind: DataVolume
      metadata:
        name: rhcos-control1-disk-0
      spec:
        pvc:
          accessModes:
            - ReadWriteMany
          resources:
            requests:
              storage: 120Gi
          storageClassName: ocs-storagecluster-ceph-rbd
          volumeMode: Block
        source:
          pvc:
            name: rhcos-dv
            namespace: ocp-tenant1
  running: true
  template:
    metadata:
      labels:
        kubevirt.io/domain: control1
        kubevirt.io/size: large
        os.template.kubevirt.io/rhel8.3: 'true'
        vm.kubevirt.io/name: control1
        workload.template.kubevirt.io/server: 'true'
    spec:
      domain:
        cpu:
          cores: 4
          sockets: 1
          threads: 1
        devices:
          autoattachPodInterface: false
          disks:
            - disk:
                bus: virtio
              name: cloudinitdisk
            - bootOrder: 1
              disk:
                bus: virtio
              name: disk-0
          interfaces:
            - bridge: {}
              macAddress: '00:00:00:00:00:00' # substitute with your MAC address
              model: virtio
              name: nic-0
          networkInterfaceMultiqueue: true
          rng: {}
        machine:
          type: pc-q35-rhel8.2.0
        resources:
          requests:
            memory: 16Gi
            cpu: "4"
      evictionStrategy: LiveMigrate
      hostname: control1
      networks:
        - multus:
            networkName: br1
          name: nic-0
      terminationGracePeriodSeconds: 90
      volumes:
        - dataVolume:
            name: rhcos-control1-disk-0
          name: disk-0
        - cloudInitConfigDrive:
            secretRef:
              name: master-ign
              # Bootstrap node -> bootstrap-ign
              # Control node -> master-ign
              # Worker node -> worker-ign
          name: cloudinitdisk
```

 The DV clones the RHCOS DV we created earlier. The process is quite fast and efficient.

1. For each node change the MAC address to the ones defined earlier in your DHCP service.

2. Choose the appropriate secret for the node type (bootstrap, master, worker)

3. Update the VM name to the appropriate name for the node. The same applies for the DV name.

4. Repeat the process for all the nodes.

5. Apply the VM YAMLs and you should see the VMs being created.


## Conclusion

As you can see, other than a few unique steps, deploying an O^3 cluster is just like deploying any other OpenShift cluster. The process is easily automatable as demonstrated in [this git repository](https://github.com/arvin-a/openshift-upi-automation).
